{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5140550,"sourceType":"datasetVersion","datasetId":2534241}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CUB 200-2001- DAAL DMAL using DIfferent Models","metadata":{}},{"cell_type":"markdown","source":"## 1. DATA PREPROCESS","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:02:23.888188Z","iopub.execute_input":"2025-05-10T20:02:23.888701Z","iopub.status.idle":"2025-05-10T20:02:24.161791Z","shell.execute_reply.started":"2025-05-10T20:02:23.888671Z","shell.execute_reply":"2025-05-10T20:02:24.161009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bouding_boxes_image_path = \"/kaggle/input/cub2002011/CUB_200_2011/bounding_boxes.txt\" #1 60.0 27.0 325.0 304.0\nclasses_image_path = \"/kaggle/input/cub2002011/CUB_200_2011/image_class_labels.txt\" # 2 002.Laysan_Albatross\nimage_path_file = \"/kaggle/input/cub2002011/CUB_200_2011/images.txt\" #1 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\ntrain_test_file = \"/kaggle/input/cub2002011/CUB_200_2011/train_test_split.txt\" #1 0\n\nfile_paths = [\n    image_path_file,\n    train_test_file,\n    bouding_boxes_image_path,\n    classes_image_path,\n]\n# Open all files at once\nfiles = [open(file_path, 'r') for file_path in file_paths]\n# # Initialize an empty list to store the line-by-line extracted info\ndataset = {}\n# # Loop through the files line by line simultaneously\n# max_class_names = 20\n\n#only select 10 for each class now\ntry:\n    while True:\n        # Read one line from each file\n        lines = [(file.name, file.readline().strip()) for file in files]\n#         print(len(lines))\n        # If any file reaches the end, break the loop\n        if any(line == '' for _, line in lines):\n            break\n#         is_training_set = lines[0][1].split(\" \")[0] == '1'\n        class_name = lines[3][1].split(\" \")[1]\n        dataset[lines[0][1].split(\" \")[1]] = {\n            \"is_training\": True if lines[1][1].split(\" \")[1] == '1' else False,\n            \"bounding_boxes\": lines[2][1].split(\" \")[1:],\n            \"class_name\": class_name\n        }\n\n        \nfinally:\n    # Make sure to close all files after reading\n    for file in files:\n        file.close()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:02:25.882248Z","iopub.execute_input":"2025-05-10T20:02:25.882787Z","iopub.status.idle":"2025-05-10T20:02:26.192879Z","shell.execute_reply.started":"2025-05-10T20:02:25.882764Z","shell.execute_reply":"2025-05-10T20:02:26.192249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### count the datasets\ntraining_dataset_count = 0\ntesting_dataset_count = 0\nclass_counts = {}\nmax_count = 0\nmin_count = 0\nfor key,value in dataset.items():\n    if value[\"is_training\"] == True:\n        training_dataset_count +=1\n    else:\n        testing_dataset_count +=1\n    class_name = value[\"class_name\"]\n    if class_name in class_counts.keys():\n        class_counts[class_name] += 1\n    else:\n        class_counts[class_name] = 0\nmax_class = max(class_counts, key=class_counts.get)\nmin_class = min(class_counts, key=class_counts.get)\nprint(\"Training count={0}\".format(training_dataset_count))\nprint(\"Testing count={0}\".format(testing_dataset_count))\n### create embeddings for cub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:02:28.392541Z","iopub.execute_input":"2025-05-10T20:02:28.392812Z","iopub.status.idle":"2025-05-10T20:02:28.403948Z","shell.execute_reply.started":"2025-05-10T20:02:28.392791Z","shell.execute_reply":"2025-05-10T20:02:28.403186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_image_with_bounding_box(image_cv, bounding_boxes):\n    x, y, w, h = bounding_boxes\n    # Crop the image using NumPy slicing\n    cropped_image = image_cv[y:y+h, x:x+w]\n    return cropped_image\n\ndef read_image_return_cropped(image_path, bounding_boxes, target_size=(224, 224)):\n    # Step 1: Load the image using OpenCV\n    image_cv = cv2.imread(image_path)\n\n    # Step 2: Convert BGR to RGB (OpenCV loads images in BGR by default)\n    image_cv_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n\n    # Step 3: Crop the image using the bounding box\n    cropped_image_cv = crop_image_with_bounding_box(image_cv_rgb, bounding_boxes)\n\n    # Step 4: Resize the cropped image to the target size using cv2\n    cropped_image_resized = cv2.resize(cropped_image_cv, target_size)\n\n    # Step 5: Check if the image is grayscale and convert to RGB if necessary\n    if len(cropped_image_resized.shape) == 2 or cropped_image_resized.shape[2] == 1:\n        cropped_image_resized = cv2.cvtColor(cropped_image_resized, cv2.COLOR_GRAY2RGB)\n\n    # Step 6: Preprocess the image for VGG16 model (normalize pixel values)\n    return cropped_image_resized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:02:32.005060Z","iopub.execute_input":"2025-05-10T20:02:32.005638Z","iopub.status.idle":"2025-05-10T20:02:32.010564Z","shell.execute_reply.started":"2025-05-10T20:02:32.005612Z","shell.execute_reply":"2025-05-10T20:02:32.009977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_train = []\nlabels_train = []\nimages_test = []\nlabels_test = []\nmain_path = \"/kaggle/input/cub2002011/CUB_200_2011/images/\"\ncount = 0\nfor key,value in dataset.items():\n    (x,y,w,h) = value['bounding_boxes']\n    is_training = value[\"is_training\"]\n    class_label = value[\"class_name\"]\n    image_path = os.path.join(main_path, key)\n    cropped_image = read_image_return_cropped(image_path, (int(float(x)), int(float(y)), int(float(w)), int(float(h))))\n    if is_training: \n        images_train.append(cropped_image)\n        labels_train.append(class_label)\n    else:\n        images_test.append(cropped_image)\n        labels_test.append(class_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:02:34.227624Z","iopub.execute_input":"2025-05-10T20:02:34.228282Z","iopub.status.idle":"2025-05-10T20:04:53.127940Z","shell.execute_reply.started":"2025-05-10T20:02:34.228250Z","shell.execute_reply":"2025-05-10T20:04:53.127262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = np.array(images_train)\ny_train = np.array(labels_train)\nX_temp = np.array(images_test)\ny_temp = np.array(labels_test)\n\n# Verify the shapes\nprint(f'Images shape: {X_train.shape}')  # (num_images, 224, 224, 3)\nprint(f'Labels shape: {y_train.shape}')  # (num_images,)\nprint(f'Images shape: {X_temp.shape}')  # (num_images, 224, 224, 3)\nprint(f'Labels shape: {y_temp.shape}')  # (num_images,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:04:57.602680Z","iopub.execute_input":"2025-05-10T20:04:57.603386Z","iopub.status.idle":"2025-05-10T20:04:58.134787Z","shell.execute_reply.started":"2025-05-10T20:04:57.603359Z","shell.execute_reply":"2025-05-10T20:04:58.134146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg19 import preprocess_input\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\n# Assuming images_np and labels_np are provided\n# images_np.shape = (num_samples, height, width, channels)\n# labels_np.shape = (num_samples,)\none_hot_encoder = OneHotEncoder(sparse=False)\n\n# Preprocess images for VGG19\nX_train = preprocess_input(X_train)\ny_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n\nX_temp = preprocess_input(X_temp)\ny_temp = one_hot_encoder.fit_transform(y_temp.reshape(-1, 1))\n# One-hot encode labels\n# labels_np_onehot = one_hot_encoder.fit_transform(labels_np.reshape(-1, 1))\n\nX_test, X_val, y_test, y_val = train_test_split(\n    X_temp, y_temp, \n    test_size=0.2,  # You can adjust the size of the validation set\n    random_state=42  # Ensures reproducibility\n)\n\n\nprint(f\"Train set: {X_train.shape}, {y_train.shape}\")\nprint(f\"test set: {X_test.shape}, {y_test.shape}\")\nprint(f\"validation set: {X_val.shape}, {y_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:05:00.130880Z","iopub.execute_input":"2025-05-10T20:05:00.131506Z","iopub.status.idle":"2025-05-10T20:05:07.531029Z","shell.execute_reply.started":"2025-05-10T20:05:00.131482Z","shell.execute_reply":"2025-05-10T20:05:07.530325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. MODEL","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nclass CenterLossLayer(tf.keras.layers.Layer):\n    def __init__(self, num_classes, embedding_dim, margin=1.5, alpha=0.001, **kwargs):\n        super(CenterLossLayer, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        self.embedding_dim = embedding_dim\n        self.margin = tf.cast(margin, tf.float32)\n        self.alpha = alpha  # EMA smoothing factor\n        self.distance = 0.1  # The length between P1 and P2\n\n        # Initialize center_p1 randomly using a normal distribution\n        self.centers_P1 = self.add_weight(name='centers_P1',\n                                          shape=(num_classes, embedding_dim),\n                                          initializer='random_normal',\n                                          trainable=False,\n                                          dtype=tf.float32)\n\n        # Create center_p2 without initialization\n        self.centers_P2 = self.add_weight(name='centers_P2',\n                                          shape=(num_classes, embedding_dim),\n                                          initializer='zeros',\n                                          trainable=False,\n                                          dtype=tf.float32)\n\n    def build(self, input_shape):\n        # Generate a random unit vector direction for center_p2\n        random_direction = tf.random.normal((self.num_classes, self.embedding_dim))\n        unit_vector = random_direction / tf.norm(random_direction, axis=1, keepdims=True)  # Normalize\n\n        # Assign center_p2 to be 'distance' units away from center_p1\n        center_p2_value = self.distance * unit_vector\n        self.centers_P2.assign(center_p2_value)\n\n        super(CenterLossLayer, self).build(input_shape)\n    \n    def call(self, inputs):\n        embeddings, labels = inputs\n\n        # Ensure embeddings and labels are float32\n        embeddings = tf.cast(embeddings, tf.float32)\n        labels = tf.argmax(labels, axis=-1, output_type=tf.int32)\n\n        # Step 1: Compute the midpoint for each class (mean of embeddings for each class)\n        batch_midpoint = tf.math.unsorted_segment_mean(embeddings, labels, num_segments=self.num_classes)\n\n        # Step 2: Calculate the variance for each class\n        squared_diff = tf.square(embeddings - tf.gather(batch_midpoint, labels))\n        batch_variance = tf.math.unsorted_segment_mean(squared_diff, labels, num_segments=self.num_classes)\n\n        # Step 3: Compute the standard deviation (sqrt of variance)\n        batch_stddev = tf.sqrt(batch_variance)\n\n        # Step 4: Position centers_P1 and centers_P2 around the midpoint\n        # Center_P1 closer to the midpoint, Center_P2 further from the midpoint\n        batch_centers_P1 = batch_midpoint - 0.5 * batch_stddev  # Center_P1 inside dense region\n        batch_centers_P2 = batch_midpoint + 0.5 * batch_stddev  # Center_P2 outside dense region\n\n        # Gather the centers corresponding to the labels\n        batch_centers_P1_gathered = tf.gather(batch_centers_P1, labels)\n        batch_centers_P2_gathered = tf.gather(batch_centers_P2, labels)\n\n        # Step 5: Update centers using EMA (Exponential Moving Average)\n        center_updates_P1 = tf.scatter_nd(tf.expand_dims(labels, 1),\n                                          batch_centers_P1_gathered,\n                                          shape=tf.shape(self.centers_P1))\n        center_updates_P2 = tf.scatter_nd(tf.expand_dims(labels, 1),\n                                          batch_centers_P2_gathered,\n                                          shape=tf.shape(self.centers_P2))\n\n        # EMA update for centers_P1 and centers_P2\n        new_centers_P1 = self.centers_P1 * (1 - self.alpha) + center_updates_P1 * self.alpha\n        new_centers_P2 = self.centers_P2 * (1 - self.alpha) + center_updates_P2 * self.alpha\n\n        # Assign updated centers\n        self.centers_P1.assign(new_centers_P1)\n        self.centers_P2.assign(new_centers_P2)\n\n        # Step 6: Compute distances to all class segments for each embedding\n        distances = self.compute_distance_to_segment_all_classes(embeddings)\n\n        # Step 7: Get the correct distances by indexing with labels\n        correct_distances = tf.gather_nd(distances, tf.expand_dims(labels, axis=-1), batch_dims=1)\n\n        # Step 8: Mask out correct class distances and find the minimum incorrect distance\n        mask = tf.one_hot(labels, depth=self.num_classes, on_value=False, off_value=True)\n        masked_distances = tf.where(mask, distances, tf.fill(tf.shape(distances), float('inf')))\n        min_incorrect_distances = tf.reduce_min(masked_distances, axis=1)\n\n        # Step 9: Compute the loss\n        incorrect_loss = tf.maximum(0.0, self.margin - min_incorrect_distances)\n        center_loss = tf.reduce_mean(tf.square(correct_distances))\n\n        return center_loss + tf.reduce_mean(incorrect_loss)\n\n\n    def compute_distance_to_segment_all_classes(self, embeddings):\n        \"\"\"\n        Compute the Euclidean distance from each embedding to the nearest point on the line segment\n        defined by P1 and P2 for each class.\n        \"\"\"\n        # Get P1 and P2 for all classes\n        P1 = self.centers_P1\n        P2 = self.centers_P2\n\n        # Vector from P1 to P2 for all classes\n        P1_P2 = P2 - P1\n\n        # Expand dims for broadcasting\n        P1 = tf.expand_dims(P1, axis=0)\n        P2 = tf.expand_dims(P2, axis=0)\n        P1_P2 = tf.expand_dims(P1_P2, axis=0)\n        embeddings = tf.expand_dims(embeddings, axis=1)\n\n        # Vector from P1 to the embeddings\n        P1_emb = embeddings - P1\n\n        # Project embeddings onto the line segment\n        proj = tf.reduce_sum(P1_emb * P1_P2, axis=2, keepdims=True) / tf.maximum(tf.reduce_sum(P1_P2 ** 2, axis=2, keepdims=True), 1e-8)\n\n        # Clamp projection to the range [0, 1] to restrict to the segment\n        proj_clamped = tf.clip_by_value(proj, 0.0, 1.0)\n\n        # Compute the nearest point on the line segment\n        nearest_point = P1 + proj_clamped * P1_P2\n\n        # Compute the Euclidean distance to the nearest point on the segment\n        distances = tf.norm(embeddings - nearest_point, axis=2)\n\n        return distances\ndef build_googlenet_center_loss(input_shape, num_classes, embedding_dim, dropout_rate, weight_decay):\n    from tensorflow.keras.applications import InceptionV3\n\n    # Define input layers\n    inputs = layers.Input(shape=input_shape, name='input')\n    labels_input = layers.Input(shape=(num_classes,), name='labels_input', dtype='float32')\n\n    # Load InceptionV3 as the backbone\n    base_model = InceptionV3(include_top=False, input_shape=input_shape, weights='imagenet')\n\n    # Optionally freeze layers (e.g., first 100)\n    for layer in base_model.layers[:100]:\n        layer.trainable = False\n\n    # Flatten and add dense layers on top\n    x = base_model.output\n    x = layers.GlobalAveragePooling2D()(x)  # InceptionV3 typically uses GAP\n    x = layers.Dense(1024, activation='swish', kernel_regularizer=regularizers.l2(weight_decay))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(512, activation='swish', kernel_regularizer=regularizers.l2(weight_decay))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(embedding_dim, activation='swish')(x)  # Final embedding layer\n\n    # Define the embedding model\n    embedding_model = models.Model(inputs=base_model.input, outputs=x, name='embedding_model_googlenet')\n\n    # Generate embeddings\n    embedding = embedding_model(inputs)\n\n    # Classification layer\n    logits = layers.Dense(num_classes, activation='softmax', name='classification_layer')(embedding)\n\n    # Center loss\n    center_loss_layer = CenterLossLayer(num_classes=num_classes, embedding_dim=embedding_dim)\n    center_loss_output = center_loss_layer([embedding, labels_input])\n\n    # Full model\n    full_model = models.Model(\n        inputs=[inputs, labels_input],\n        outputs=[logits, center_loss_output],\n        name='full_model_googlenet'\n    )\n\n    return embedding_model, full_model\n\n\ndef build_vgg19_center_loss(input_shape, num_classes, embedding_dim, dropout_rate, weight_decay):\n    # Define input layers\n    inputs = layers.Input(shape=input_shape, name='input')\n    labels_input = layers.Input(shape=(num_classes,), name='labels_input', dtype='float32')\n\n    # Define VGG19 backbone (without the top layers)\n    base_model = tf.keras.applications.VGG19(include_top=False, input_shape=input_shape, weights='imagenet')\n\n    for layer in base_model.layers[:10]:  # Freeze the first 15 layers (you can adjust this number)\n        layer.trainable = False\n    \n    # Flatten the output of the VGG19 backbone\n    x = base_model.output\n    x = layers.Flatten()(x)\n    \n    x = layers.Dense(1024, activation='swish', kernel_regularizer=regularizers.l2(weight_decay), bias_regularizer=regularizers.l2(0.01) )(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(512, activation='swish', kernel_regularizer=regularizers.l2(weight_decay), bias_regularizer=regularizers.l2(0.01) )(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(512, activation='swish', kernel_regularizer=regularizers.l2(weight_decay), bias_regularizer=regularizers.l2(0.01) )(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(1024, activation='swish', kernel_regularizer=regularizers.l2(weight_decay), bias_regularizer=regularizers.l2(0.01) )(x)\n    x = layers.Dense(embedding_dim, activation='swish')(x)\n\n    # Define the embedding model\n    embedding_model = models.Model(inputs=base_model.input, outputs=x, name='embedding_model')\n\n    # Generate embeddings for the inputs\n    embedding = embedding_model(inputs)\n\n    # Define logits for classification using the embedding\n    logits = layers.Dense(num_classes, activation='softmax', name='classification_layer')(embedding)\n    # logits = tf.keras.layers.Lambda(lambda x: x / 10)(logits)  # Example scaling\n    # logits = tf.keras.layers.Activation('softmax')(logits)\n\n    # Define the custom center loss layer\n    center_loss_layer = CenterLossLayer(num_classes=num_classes, embedding_dim=embedding_dim)\n    center_loss_output = center_loss_layer([embedding, labels_input])\n\n    # Define the full model with classification and center loss\n    full_model = models.Model(\n        inputs=[inputs, labels_input],\n        outputs=[logits, center_loss_output],\n        name='full_model'\n    )\n\n    return embedding_model, full_model\n\ndef train_and_evaluate(model, train_generator, val_generator, steps_per_epoch, validation_steps, epochs, center_loss_weight, learning_rate):\n    optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n\n    history = {\n        \"train_loss\": [],\n        \"train_class_loss\": [],\n        \"train_center_loss\": [],\n        \"train_acc\": [],\n        \"val_class_loss\": [],\n        \"val_center_loss\": [],\n        \"val_acc\": []\n    }\n\n    @tf.function\n    def train_step(inputs, labels):\n        with tf.GradientTape() as tape:\n            logits, center_loss = model([inputs, labels], training=True)\n            classification_loss = tf.keras.losses.CategoricalCrossentropy()(labels, logits)\n            total_loss = classification_loss + center_loss_weight * center_loss\n\n        gradients = tape.gradient(total_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        predictions = tf.argmax(logits, axis=-1)\n        labels_true = tf.argmax(labels, axis=-1)\n        train_acc = tf.reduce_mean(tf.cast(tf.equal(predictions, labels_true), tf.float32))\n\n        return total_loss, classification_loss, center_loss, train_acc\n\n    @tf.function\n    def eval_step(inputs, labels):\n        logits, center_loss = model([inputs, labels], training=False)\n        classification_loss = tf.keras.losses.CategoricalCrossentropy()(labels, logits)\n\n        predictions = tf.argmax(logits, axis=-1)\n        labels_true = tf.argmax(labels, axis=-1)\n        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels_true), tf.float32))\n\n        return classification_loss, center_loss, accuracy\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n\n        # Training loop\n        epoch_loss, epoch_class_loss, epoch_center_loss, epoch_acc = 0, 0, 0, 0\n        for step in range(steps_per_epoch):\n            inputs_batch, labels_batch = next(train_generator)\n            loss, class_loss, center_loss, acc = train_step(inputs_batch, labels_batch)\n            epoch_loss += loss\n            epoch_class_loss += class_loss\n            epoch_center_loss += center_loss\n            epoch_acc += acc\n\n        epoch_loss /= steps_per_epoch\n        epoch_class_loss /= steps_per_epoch\n        epoch_center_loss /= steps_per_epoch\n        epoch_acc /= steps_per_epoch\n\n        print(f\"Train Loss: {epoch_loss:.4f}, Class Loss: {epoch_class_loss:.4f}, Center Loss: {epoch_center_loss:.4f}, Acc: {epoch_acc:.4f}\")\n\n        history[\"train_loss\"].append(epoch_loss)\n        history[\"train_class_loss\"].append(epoch_class_loss)\n        history[\"train_center_loss\"].append(epoch_center_loss)\n        history[\"train_acc\"].append(epoch_acc)\n\n        # Validation loop\n        val_class_loss, val_center_loss, val_acc = 0, 0, 0\n        for step in range(validation_steps):\n            inputs_batch, labels_batch = next(val_generator)\n            class_loss, center_loss, acc = eval_step(inputs_batch, labels_batch)\n            val_class_loss += class_loss\n            val_center_loss += center_loss\n            val_acc += acc\n\n        val_class_loss /= validation_steps\n        val_center_loss /= validation_steps\n        val_acc /= validation_steps\n\n        print(f\"Val Class Loss: {val_class_loss:.4f}, Val Center Loss: {val_center_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        history[\"val_class_loss\"].append(val_class_loss)\n        history[\"val_center_loss\"].append(val_center_loss)\n        history[\"val_acc\"].append(val_acc)\n\n    return history\n\n# Initialize ImageDataGenerator with augmentation options (without rescaling)\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,           # Randomly rotate images by 20 degrees\n    width_shift_range=0.2,       # Randomly shift images horizontally\n    height_shift_range=0.2,      # Randomly shift images vertically\n    shear_range=0.2,             # Shear transformation\n    zoom_range=0.2,              # Zoom in/out\n    horizontal_flip=True,        # Random horizontal flipping\n    fill_mode='nearest'          # Filling pixels after transformations\n)\n\n# Example usage with specified values\ninput_shape = (224, 224, 3)        # Input shape for images (224x224 RGB)\nnum_classes = y_train.shape[1]                  # Number of classes in the dataset\nembedding_dim = 1000              # Dimensionality of the embedding space\ndropout_rate = 0.2               # Dropout rate for regularization\nweight_decay = 0.05 #0.005            # L2 regularization weight\ncenter_loss_weight = 0.01 #0.0001          # Weight for center loss\nlearning_rate = 1e-4               # Learning rate for the optimizer\nbatch_size = 64                    # Batch size for training\nepochs = 70                      # Number of epochs to train\n\nval_datagen = ImageDataGenerator()  # No additional augmentations for validation\n\n# Load and augment training data\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n\n# Load validation data\nval_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n\nsteps_per_epoch = len(X_train) // batch_size\nvalidation_steps = len(X_val)  // batch_size\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:05:11.467957Z","iopub.execute_input":"2025-05-10T20:05:11.468775Z","iopub.status.idle":"2025-05-10T20:05:11.501721Z","shell.execute_reply.started":"2025-05-10T20:05:11.468749Z","shell.execute_reply":"2025-05-10T20:05:11.500953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Build the model using VGG19 and center loss\nembedding_model, full_model = build_googlenet_center_loss(input_shape, num_classes, embedding_dim, dropout_rate, weight_decay)\n# Train the model using data generators\nhistory = train_and_evaluate(full_model, train_generator, val_generator, steps_per_epoch, validation_steps, epochs, center_loss_weight, learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:05:17.095671Z","iopub.execute_input":"2025-05-10T20:05:17.095944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Save Model","metadata":{}},{"cell_type":"code","source":"# Save the history to a .pkl file\nimport pickle\nwith open('/kaggle/working/cub_history_standard_googleNet.pkl', 'wb') as file:\n    pickle.dump(history, file)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:42:14.048362Z","iopub.execute_input":"2025-05-10T18:42:14.049103Z","iopub.status.idle":"2025-05-10T18:42:14.055738Z","shell.execute_reply.started":"2025-05-10T18:42:14.049078Z","shell.execute_reply":"2025-05-10T18:42:14.055049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the embedding model\nembedding_model.save('/kaggle/working/cub_embedding_model_standard_googleNet.keras')\n\n# Save the full model\nfull_model.save('/kaggle/working/cub_full_model_standard_googleNet.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T18:44:34.655743Z","iopub.execute_input":"2025-05-10T18:44:34.656012Z","iopub.status.idle":"2025-05-10T18:44:36.314617Z","shell.execute_reply.started":"2025-05-10T18:44:34.655993Z","shell.execute_reply":"2025-05-10T18:44:36.313815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Metric","metadata":{}}]}