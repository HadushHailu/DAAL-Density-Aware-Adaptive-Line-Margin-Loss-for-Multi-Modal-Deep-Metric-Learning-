{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5140550,"sourceType":"datasetVersion","datasetId":2534241}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Dataset prepration","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:13:50.065227Z","iopub.execute_input":"2025-05-11T01:13:50.065722Z","iopub.status.idle":"2025-05-11T01:13:50.616315Z","shell.execute_reply.started":"2025-05-11T01:13:50.065703Z","shell.execute_reply":"2025-05-11T01:13:50.615471Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"bouding_boxes_image_path = \"/kaggle/input/cub2002011/CUB_200_2011/bounding_boxes.txt\" #1 60.0 27.0 325.0 304.0\nclasses_image_path = \"/kaggle/input/cub2002011/CUB_200_2011/image_class_labels.txt\" # 2 002.Laysan_Albatross\nimage_path_file = \"/kaggle/input/cub2002011/CUB_200_2011/images.txt\" #1 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\ntrain_test_file = \"/kaggle/input/cub2002011/CUB_200_2011/train_test_split.txt\" #1 0\n\nfile_paths = [\n    image_path_file,\n    train_test_file,\n    bouding_boxes_image_path,\n    classes_image_path,\n]\n# Open all files at once\nfiles = [open(file_path, 'r') for file_path in file_paths]\n# # Initialize an empty list to store the line-by-line extracted info\ndataset = {}\n# # Loop through the files line by line simultaneously\n# max_class_names = 20\n\n#only select 10 for each class now\ntry:\n    while True:\n        # Read one line from each file\n        lines = [(file.name, file.readline().strip()) for file in files]\n#         print(len(lines))\n        # If any file reaches the end, break the loop\n        if any(line == '' for _, line in lines):\n            break\n#         is_training_set = lines[0][1].split(\" \")[0] == '1'\n        class_name = lines[3][1].split(\" \")[1]\n        dataset[lines[0][1].split(\" \")[1]] = {\n            \"is_training\": True if lines[1][1].split(\" \")[1] == '1' else False,\n            \"bounding_boxes\": lines[2][1].split(\" \")[1:],\n            \"class_name\": class_name\n        }\n\n        \nfinally:\n    # Make sure to close all files after reading\n    for file in files:\n        file.close()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:13:54.294899Z","iopub.execute_input":"2025-05-11T01:13:54.295737Z","iopub.status.idle":"2025-05-11T01:13:54.375096Z","shell.execute_reply.started":"2025-05-11T01:13:54.295710Z","shell.execute_reply":"2025-05-11T01:13:54.374365Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"### count the datasets\ntraining_dataset_count = 0\ntesting_dataset_count = 0\nclass_counts = {}\nmax_count = 0\nmin_count = 0\nfor key,value in dataset.items():\n    if value[\"is_training\"] == True:\n        training_dataset_count +=1\n    else:\n        testing_dataset_count +=1\n    class_name = value[\"class_name\"]\n    if class_name in class_counts.keys():\n        class_counts[class_name] += 1\n    else:\n        class_counts[class_name] = 0\nmax_class = max(class_counts, key=class_counts.get)\nmin_class = min(class_counts, key=class_counts.get)\nprint(\"Training count={0}\".format(training_dataset_count))\nprint(\"Testing count={0}\".format(testing_dataset_count))\n### create embeddings for cub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:13:56.590967Z","iopub.execute_input":"2025-05-11T01:13:56.591436Z","iopub.status.idle":"2025-05-11T01:13:56.604836Z","shell.execute_reply.started":"2025-05-11T01:13:56.591413Z","shell.execute_reply":"2025-05-11T01:13:56.604068Z"}},"outputs":[{"name":"stdout","text":"Training count=5994\nTesting count=5794\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def crop_image_with_bounding_box(image_cv, bounding_boxes):\n    x, y, w, h = bounding_boxes\n    # Crop the image using NumPy slicing\n    cropped_image = image_cv[y:y+h, x:x+w]\n    return cropped_image\n\ndef read_image_return_cropped(image_path, bounding_boxes, target_size=(224, 224)):\n    # Step 1: Load the image using OpenCV\n    image_cv = cv2.imread(image_path)\n\n    # Step 2: Convert BGR to RGB (OpenCV loads images in BGR by default)\n    image_cv_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n\n    # Step 3: Crop the image using the bounding box\n    cropped_image_cv = crop_image_with_bounding_box(image_cv_rgb, bounding_boxes)\n\n    # Step 4: Resize the cropped image to the target size using cv2\n    cropped_image_resized = cv2.resize(cropped_image_cv, target_size)\n\n    # Step 5: Check if the image is grayscale and convert to RGB if necessary\n    if len(cropped_image_resized.shape) == 2 or cropped_image_resized.shape[2] == 1:\n        cropped_image_resized = cv2.cvtColor(cropped_image_resized, cv2.COLOR_GRAY2RGB)\n\n    # Step 6: Preprocess the image for VGG16 model (normalize pixel values)\n    return cropped_image_resized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:13:59.060643Z","iopub.execute_input":"2025-05-11T01:13:59.060901Z","iopub.status.idle":"2025-05-11T01:13:59.066464Z","shell.execute_reply.started":"2025-05-11T01:13:59.060882Z","shell.execute_reply":"2025-05-11T01:13:59.065707Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"images_train = []\nlabels_train = []\nimages_test = []\nlabels_test = []\nmain_path = \"/kaggle/input/cub2002011/CUB_200_2011/images/\"\ncount = 0\nfor key,value in dataset.items():\n    (x,y,w,h) = value['bounding_boxes']\n    is_training = value[\"is_training\"]\n    class_label = value[\"class_name\"]\n    image_path = os.path.join(main_path, key)\n    cropped_image = read_image_return_cropped(image_path, (int(float(x)), int(float(y)), int(float(w)), int(float(h))))\n    if is_training: \n        images_train.append(cropped_image)\n        labels_train.append(class_label)\n    else:\n        images_test.append(cropped_image)\n        labels_test.append(class_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:14:02.610740Z","iopub.execute_input":"2025-05-11T01:14:02.611002Z","iopub.status.idle":"2025-05-11T01:16:00.234831Z","shell.execute_reply.started":"2025-05-11T01:14:02.610981Z","shell.execute_reply":"2025-05-11T01:16:00.234177Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"X_train = np.array(images_train)\ny_train = np.array(labels_train)\nX_temp = np.array(images_test)\ny_temp = np.array(labels_test)\n\n# Verify the shapes\nprint(f'Images shape: {X_train.shape}')  # (num_images, 224, 224, 3)\nprint(f'Labels shape: {y_train.shape}')  # (num_images,)\nprint(f'Images shape: {X_temp.shape}')  # (num_images, 224, 224, 3)\nprint(f'Labels shape: {y_temp.shape}')  # (num_images,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:16:22.629312Z","iopub.execute_input":"2025-05-11T01:16:22.629565Z","iopub.status.idle":"2025-05-11T01:16:23.163467Z","shell.execute_reply.started":"2025-05-11T01:16:22.629541Z","shell.execute_reply":"2025-05-11T01:16:23.162701Z"}},"outputs":[{"name":"stdout","text":"Images shape: (5994, 224, 224, 3)\nLabels shape: (5994,)\nImages shape: (5794, 224, 224, 3)\nLabels shape: (5794,)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Dataset preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.applications.resnet import preprocess_input\nimport numpy as np\n\n# One-hot encode labels\none_hot_encoder = OneHotEncoder(sparse=False)\ny_train = one_hot_encoder.fit_transform(np.array(labels_train).reshape(-1, 1))\ny_temp = one_hot_encoder.transform(np.array(labels_test).reshape(-1, 1))\n\n# Preprocess images using ResNet's built-in method\nX_train = preprocess_input(np.array(images_train).astype(np.float32))\nX_temp = preprocess_input(np.array(images_test).astype(np.float32))\n\n# Split test set into validation/test\nX_test, X_val, y_test, y_val = train_test_split(\n    X_temp, y_temp,\n    test_size=0.2,\n    random_state=42\n)\n\n# Confirm shapes\nprint(f\"Train set: {X_train.shape}, {y_train.shape}\")\nprint(f\"Test set: {X_test.shape}, {y_test.shape}\")\nprint(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:16:26.066304Z","iopub.execute_input":"2025-05-11T01:16:26.066648Z","iopub.status.idle":"2025-05-11T01:16:48.575133Z","shell.execute_reply.started":"2025-05-11T01:16:26.066623Z","shell.execute_reply":"2025-05-11T01:16:48.574438Z"}},"outputs":[{"name":"stderr","text":"2025-05-11 01:16:28.722916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746926189.035426      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746926189.118985      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Train set: (5994, 224, 224, 3), (5994, 200)\nTest set: (4635, 224, 224, 3), (4635, 200)\nValidation set: (1159, 224, 224, 3), (1159, 200)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 2. MODEL","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nclass CenterLossLayer(tf.keras.layers.Layer):\n    def __init__(self, num_classes, embedding_dim, margin=1.5, alpha=0.001, **kwargs):\n        super(CenterLossLayer, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        self.embedding_dim = embedding_dim\n        self.margin = tf.cast(margin, tf.float32)\n        self.alpha = alpha  # EMA smoothing factor\n        self.distance = 0.1  # The length between P1 and P2\n\n        # Initialize center_p1 randomly using a normal distribution\n        self.centers_P1 = self.add_weight(name='centers_P1',\n                                          shape=(num_classes, embedding_dim),\n                                          initializer='random_normal',\n                                          trainable=False,\n                                          dtype=tf.float32)\n\n        # Create center_p2 without initialization\n        self.centers_P2 = self.add_weight(name='centers_P2',\n                                          shape=(num_classes, embedding_dim),\n                                          initializer='zeros',\n                                          trainable=False,\n                                          dtype=tf.float32)\n\n    def build(self, input_shape):\n        # Generate a random unit vector direction for center_p2\n        random_direction = tf.random.normal((self.num_classes, self.embedding_dim))\n        unit_vector = random_direction / tf.norm(random_direction, axis=1, keepdims=True)  # Normalize\n\n        # Assign center_p2 to be 'distance' units away from center_p1\n        center_p2_value = self.distance * unit_vector\n        self.centers_P2.assign(center_p2_value)\n\n        super(CenterLossLayer, self).build(input_shape)\n    \n    def call(self, inputs):\n        embeddings, labels = inputs\n\n        # Ensure embeddings and labels are float32\n        embeddings = tf.cast(embeddings, tf.float32)\n        labels = tf.argmax(labels, axis=-1, output_type=tf.int32)\n\n        # Step 1: Compute the midpoint for each class (mean of embeddings for each class)\n        batch_midpoint = tf.math.unsorted_segment_mean(embeddings, labels, num_segments=self.num_classes)\n\n        # Step 2: Calculate the variance for each class\n        squared_diff = tf.square(embeddings - tf.gather(batch_midpoint, labels))\n        batch_variance = tf.math.unsorted_segment_mean(squared_diff, labels, num_segments=self.num_classes)\n\n        # Step 3: Compute the standard deviation (sqrt of variance)\n        batch_stddev = tf.sqrt(batch_variance)\n\n        # Step 4: Position centers_P1 and centers_P2 around the midpoint\n        # Center_P1 closer to the midpoint, Center_P2 further from the midpoint\n        batch_centers_P1 = batch_midpoint - 0.5 * batch_stddev  # Center_P1 inside dense region\n        batch_centers_P2 = batch_midpoint + 0.5 * batch_stddev  # Center_P2 outside dense region\n\n        # Gather the centers corresponding to the labels\n        batch_centers_P1_gathered = tf.gather(batch_centers_P1, labels)\n        batch_centers_P2_gathered = tf.gather(batch_centers_P2, labels)\n\n        # Step 5: Update centers using EMA (Exponential Moving Average)\n        center_updates_P1 = tf.scatter_nd(tf.expand_dims(labels, 1),\n                                          batch_centers_P1_gathered,\n                                          shape=tf.shape(self.centers_P1))\n        center_updates_P2 = tf.scatter_nd(tf.expand_dims(labels, 1),\n                                          batch_centers_P2_gathered,\n                                          shape=tf.shape(self.centers_P2))\n\n        # EMA update for centers_P1 and centers_P2\n        new_centers_P1 = self.centers_P1 * (1 - self.alpha) + center_updates_P1 * self.alpha\n        new_centers_P2 = self.centers_P2 * (1 - self.alpha) + center_updates_P2 * self.alpha\n\n        # Assign updated centers\n        self.centers_P1.assign(new_centers_P1)\n        self.centers_P2.assign(new_centers_P2)\n\n        # Step 6: Compute distances to all class segments for each embedding\n        distances = self.compute_distance_to_segment_all_classes(embeddings)\n\n        # Step 7: Get the correct distances by indexing with labels\n        correct_distances = tf.gather_nd(distances, tf.expand_dims(labels, axis=-1), batch_dims=1)\n\n        # Step 8: Mask out correct class distances and find the minimum incorrect distance\n        mask = tf.one_hot(labels, depth=self.num_classes, on_value=False, off_value=True)\n        masked_distances = tf.where(mask, distances, tf.fill(tf.shape(distances), float('inf')))\n        min_incorrect_distances = tf.reduce_min(masked_distances, axis=1)\n\n        # Step 9: Compute the loss\n        incorrect_loss = tf.maximum(0.0, self.margin - min_incorrect_distances)\n        center_loss = tf.reduce_mean(tf.square(correct_distances))\n\n        return center_loss + tf.reduce_mean(incorrect_loss)\n\n\n    def compute_distance_to_segment_all_classes(self, embeddings):\n        \"\"\"\n        Compute the Euclidean distance from each embedding to the nearest point on the line segment\n        defined by P1 and P2 for each class.\n        \"\"\"\n        # Get P1 and P2 for all classes\n        P1 = self.centers_P1\n        P2 = self.centers_P2\n\n        # Vector from P1 to P2 for all classes\n        P1_P2 = P2 - P1\n\n        # Expand dims for broadcasting\n        P1 = tf.expand_dims(P1, axis=0)\n        P2 = tf.expand_dims(P2, axis=0)\n        P1_P2 = tf.expand_dims(P1_P2, axis=0)\n        embeddings = tf.expand_dims(embeddings, axis=1)\n\n        # Vector from P1 to the embeddings\n        P1_emb = embeddings - P1\n\n        # Project embeddings onto the line segment\n        proj = tf.reduce_sum(P1_emb * P1_P2, axis=2, keepdims=True) / tf.maximum(tf.reduce_sum(P1_P2 ** 2, axis=2, keepdims=True), 1e-8)\n\n        # Clamp projection to the range [0, 1] to restrict to the segment\n        proj_clamped = tf.clip_by_value(proj, 0.0, 1.0)\n\n        # Compute the nearest point on the line segment\n        nearest_point = P1 + proj_clamped * P1_P2\n\n        # Compute the Euclidean distance to the nearest point on the segment\n        distances = tf.norm(embeddings - nearest_point, axis=2)\n\n        return distances\n        \ndef build_resnet50_center_loss(input_shape, num_classes, embedding_dim, dropout_rate, weight_decay):\n    from tensorflow.keras.applications import ResNet50\n\n    # Define input layers\n    inputs = layers.Input(shape=input_shape, name='input')\n    labels_input = layers.Input(shape=(num_classes,), name='labels_input', dtype='float32')\n\n    # Load ResNet50 as the backbone\n    base_model = ResNet50(include_top=False, input_shape=input_shape, weights='imagenet')\n\n    # Optionally freeze early layers (adjust as needed)\n    for layer in base_model.layers[:100]:\n        layer.trainable = False\n\n    # Add custom dense layers after ResNet50\n    x = base_model.output\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(1024, activation='swish', kernel_regularizer=regularizers.l2(weight_decay),\n                     bias_regularizer=regularizers.l2(0.01))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(512, activation='swish', kernel_regularizer=regularizers.l2(weight_decay),\n                     bias_regularizer=regularizers.l2(0.01))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(512, activation='swish', kernel_regularizer=regularizers.l2(weight_decay),\n                     bias_regularizer=regularizers.l2(0.01))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(1024, activation='swish', kernel_regularizer=regularizers.l2(weight_decay),\n                     bias_regularizer=regularizers.l2(0.01))(x)\n\n    # Embedding layer with specified dimension\n    x = layers.Dense(embedding_dim, activation='swish')(x)\n\n    # Embedding model\n    embedding_model = models.Model(inputs=base_model.input, outputs=x, name='embedding_model_resnet50')\n\n    # Generate embeddings\n    embedding = embedding_model(inputs)\n\n    # Classification layer\n    logits = layers.Dense(num_classes, activation='softmax', name='classification_layer')(embedding)\n\n    # Center loss\n    center_loss_layer = CenterLossLayer(num_classes=num_classes, embedding_dim=embedding_dim)\n    center_loss_output = center_loss_layer([embedding, labels_input])\n\n    # Full model\n    full_model = models.Model(\n        inputs=[inputs, labels_input],\n        outputs=[logits, center_loss_output],\n        name='full_model_resnet50'\n    )\n\n    return embedding_model, full_model\n\n\ndef train_and_evaluate(model, train_generator, val_generator, steps_per_epoch, validation_steps, epochs, center_loss_weight, learning_rate):\n    optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n\n    history = {\n        \"train_loss\": [],\n        \"train_class_loss\": [],\n        \"train_center_loss\": [],\n        \"train_acc\": [],\n        \"val_class_loss\": [],\n        \"val_center_loss\": [],\n        \"val_acc\": []\n    }\n\n    @tf.function\n    def train_step(inputs, labels):\n        with tf.GradientTape() as tape:\n            logits, center_loss = model([inputs, labels], training=True)\n            classification_loss = tf.keras.losses.CategoricalCrossentropy()(labels, logits)\n            total_loss = classification_loss + center_loss_weight * center_loss\n\n        gradients = tape.gradient(total_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        predictions = tf.argmax(logits, axis=-1)\n        labels_true = tf.argmax(labels, axis=-1)\n        train_acc = tf.reduce_mean(tf.cast(tf.equal(predictions, labels_true), tf.float32))\n\n        return total_loss, classification_loss, center_loss, train_acc\n\n    @tf.function\n    def eval_step(inputs, labels):\n        logits, center_loss = model([inputs, labels], training=False)\n        classification_loss = tf.keras.losses.CategoricalCrossentropy()(labels, logits)\n\n        predictions = tf.argmax(logits, axis=-1)\n        labels_true = tf.argmax(labels, axis=-1)\n        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels_true), tf.float32))\n\n        return classification_loss, center_loss, accuracy\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n\n        # Training loop\n        epoch_loss, epoch_class_loss, epoch_center_loss, epoch_acc = 0, 0, 0, 0\n        for step in range(steps_per_epoch):\n            inputs_batch, labels_batch = next(train_generator)\n            loss, class_loss, center_loss, acc = train_step(inputs_batch, labels_batch)\n            epoch_loss += loss\n            epoch_class_loss += class_loss\n            epoch_center_loss += center_loss\n            epoch_acc += acc\n\n        epoch_loss /= steps_per_epoch\n        epoch_class_loss /= steps_per_epoch\n        epoch_center_loss /= steps_per_epoch\n        epoch_acc /= steps_per_epoch\n\n        print(f\"Train Loss: {epoch_loss:.4f}, Class Loss: {epoch_class_loss:.4f}, Center Loss: {epoch_center_loss:.4f}, Acc: {epoch_acc:.4f}\")\n\n        history[\"train_loss\"].append(epoch_loss)\n        history[\"train_class_loss\"].append(epoch_class_loss)\n        history[\"train_center_loss\"].append(epoch_center_loss)\n        history[\"train_acc\"].append(epoch_acc)\n\n        # Validation loop\n        val_class_loss, val_center_loss, val_acc = 0, 0, 0\n        for step in range(validation_steps):\n            inputs_batch, labels_batch = next(val_generator)\n            class_loss, center_loss, acc = eval_step(inputs_batch, labels_batch)\n            val_class_loss += class_loss\n            val_center_loss += center_loss\n            val_acc += acc\n\n        val_class_loss /= validation_steps\n        val_center_loss /= validation_steps\n        val_acc /= validation_steps\n\n        print(f\"Val Class Loss: {val_class_loss:.4f}, Val Center Loss: {val_center_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        history[\"val_class_loss\"].append(val_class_loss)\n        history[\"val_center_loss\"].append(val_center_loss)\n        history[\"val_acc\"].append(val_acc)\n\n    return history\n\n# Initialize ImageDataGenerator with augmentation options (without rescaling)\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,           # Randomly rotate images by 20 degrees\n    width_shift_range=0.2,       # Randomly shift images horizontally\n    height_shift_range=0.2,      # Randomly shift images vertically\n    shear_range=0.2,             # Shear transformation\n    zoom_range=0.2,              # Zoom in/out\n    horizontal_flip=True,        # Random horizontal flipping\n    fill_mode='nearest'          # Filling pixels after transformations\n)\n\n# Example usage with specified values\ninput_shape = (224, 224, 3)        # Input shape for images (224x224 RGB)\nnum_classes = y_train.shape[1]                  # Number of classes in the dataset\nembedding_dim = 1000              # Dimensionality of the embedding space\ndropout_rate = 0.2               # Dropout rate for regularization\nweight_decay = 0.05 #0.005            # L2 regularization weight\ncenter_loss_weight = 0.01 #0.0001          # Weight for center loss\nlearning_rate = 1e-4               # Learning rate for the optimizer\nbatch_size = 64                    # Batch size for training\nepochs = 40                      # Number of epochs to train\n\nval_datagen = ImageDataGenerator()  # No additional augmentations for validation\n\n# Load and augment training data\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n\n# Load validation data\nval_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n\nsteps_per_epoch = len(X_train) // batch_size\nvalidation_steps = len(X_val)  // batch_size\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:16:53.290631Z","iopub.execute_input":"2025-05-11T01:16:53.291349Z","iopub.status.idle":"2025-05-11T01:16:53.319771Z","shell.execute_reply.started":"2025-05-11T01:16:53.291321Z","shell.execute_reply":"2025-05-11T01:16:53.318975Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Build the model using VGG19 and center loss\nembedding_model, full_model = build_resnet50_center_loss(input_shape, num_classes, embedding_dim, dropout_rate, weight_decay)\n# Train the model using data generators\nhistory = train_and_evaluate(full_model, train_generator, val_generator, steps_per_epoch, validation_steps, epochs, center_loss_weight, learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T01:17:00.359534Z","iopub.execute_input":"2025-05-11T01:17:00.359846Z","iopub.status.idle":"2025-05-11T02:21:54.117336Z","shell.execute_reply.started":"2025-05-11T01:17:00.359814Z","shell.execute_reply":"2025-05-11T02:21:54.116628Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1746926222.228620      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746926222.229358      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746926244.512600      95 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.2031, Class Loss: 5.1508, Center Loss: 5.2238, Acc: 0.0467\nVal Class Loss: 4.3424, Val Center Loss: 53.2607, Val Acc: 0.1163\nEpoch 2/40\nTrain Loss: 4.5378, Class Loss: 4.1947, Center Loss: 34.3123, Acc: 0.1805\nVal Class Loss: 3.2794, Val Center Loss: 60.0421, Val Acc: 0.3189\nEpoch 3/40\nTrain Loss: 3.8114, Class Loss: 3.2334, Center Loss: 57.7996, Acc: 0.3685\nVal Class Loss: 2.7649, Val Center Loss: 49.8709, Val Acc: 0.4908\nEpoch 4/40\nTrain Loss: 3.1944, Class Loss: 2.5119, Center Loss: 68.2497, Acc: 0.5102\nVal Class Loss: 2.2947, Val Center Loss: 54.0838, Val Acc: 0.5279\nEpoch 5/40\nTrain Loss: 2.7314, Class Loss: 2.0264, Center Loss: 70.4952, Acc: 0.6144\nVal Class Loss: 1.9447, Val Center Loss: 47.6002, Val Acc: 0.6837\nEpoch 6/40\nTrain Loss: 2.3173, Class Loss: 1.6186, Center Loss: 69.8723, Acc: 0.6895\nVal Class Loss: 1.7015, Val Center Loss: 47.1659, Val Acc: 0.6812\nEpoch 7/40\nTrain Loss: 1.9990, Class Loss: 1.3288, Center Loss: 67.0200, Acc: 0.7585\nVal Class Loss: 1.2775, Val Center Loss: 57.3225, Val Acc: 0.7360\nEpoch 8/40\nTrain Loss: 1.7294, Class Loss: 1.0949, Center Loss: 63.4544, Acc: 0.7972\nVal Class Loss: 1.2475, Val Center Loss: 46.1915, Val Acc: 0.7429\nEpoch 9/40\nTrain Loss: 1.5063, Class Loss: 0.9150, Center Loss: 59.1344, Acc: 0.8356\nVal Class Loss: 1.0956, Val Center Loss: 45.7981, Val Acc: 0.7360\nEpoch 10/40\nTrain Loss: 1.3262, Class Loss: 0.7761, Center Loss: 55.0159, Acc: 0.8635\nVal Class Loss: 1.0178, Val Center Loss: 43.2386, Val Acc: 0.7603\nEpoch 11/40\nTrain Loss: 1.1715, Class Loss: 0.6604, Center Loss: 51.1082, Acc: 0.8842\nVal Class Loss: 1.0181, Val Center Loss: 36.8721, Val Acc: 0.7697\nEpoch 12/40\nTrain Loss: 1.0395, Class Loss: 0.5638, Center Loss: 47.5704, Acc: 0.9005\nVal Class Loss: 0.9742, Val Center Loss: 37.4386, Val Acc: 0.7540\nEpoch 13/40\nTrain Loss: 0.9253, Class Loss: 0.4823, Center Loss: 44.3013, Acc: 0.9208\nVal Class Loss: 0.9792, Val Center Loss: 34.7135, Val Acc: 0.7532\nEpoch 14/40\nTrain Loss: 0.8319, Class Loss: 0.4186, Center Loss: 41.3346, Acc: 0.9342\nVal Class Loss: 0.9598, Val Center Loss: 32.1963, Val Acc: 0.7742\nEpoch 15/40\nTrain Loss: 0.7521, Class Loss: 0.3648, Center Loss: 38.7265, Acc: 0.9479\nVal Class Loss: 0.9154, Val Center Loss: 32.5167, Val Acc: 0.7750\nEpoch 16/40\nTrain Loss: 0.6753, Class Loss: 0.3116, Center Loss: 36.3614, Acc: 0.9566\nVal Class Loss: 0.8403, Val Center Loss: 28.1953, Val Acc: 0.7924\nEpoch 17/40\nTrain Loss: 0.6195, Class Loss: 0.2806, Center Loss: 33.8923, Acc: 0.9641\nVal Class Loss: 0.8311, Val Center Loss: 27.9266, Val Acc: 0.7908\nEpoch 18/40\nTrain Loss: 0.5614, Class Loss: 0.2432, Center Loss: 31.8259, Acc: 0.9692\nVal Class Loss: 0.8289, Val Center Loss: 27.6250, Val Acc: 0.7862\nEpoch 19/40\nTrain Loss: 0.5257, Class Loss: 0.2243, Center Loss: 30.1403, Acc: 0.9733\nVal Class Loss: 0.8774, Val Center Loss: 24.2159, Val Acc: 0.7697\nEpoch 20/40\nTrain Loss: 0.4850, Class Loss: 0.2015, Center Loss: 28.3431, Acc: 0.9752\nVal Class Loss: 0.8735, Val Center Loss: 25.6406, Val Acc: 0.7786\nEpoch 21/40\nTrain Loss: 0.4467, Class Loss: 0.1772, Center Loss: 26.9513, Acc: 0.9822\nVal Class Loss: 0.8987, Val Center Loss: 21.8372, Val Acc: 0.7820\nEpoch 22/40\nTrain Loss: 0.4188, Class Loss: 0.1629, Center Loss: 25.5914, Acc: 0.9839\nVal Class Loss: 0.8751, Val Center Loss: 21.5379, Val Acc: 0.7749\nEpoch 23/40\nTrain Loss: 0.3970, Class Loss: 0.1530, Center Loss: 24.4061, Acc: 0.9845\nVal Class Loss: 0.9513, Val Center Loss: 20.3706, Val Acc: 0.7722\nEpoch 24/40\nTrain Loss: 0.3665, Class Loss: 0.1343, Center Loss: 23.2165, Acc: 0.9892\nVal Class Loss: 0.8201, Val Center Loss: 18.7703, Val Acc: 0.7959\nEpoch 25/40\nTrain Loss: 0.3492, Class Loss: 0.1283, Center Loss: 22.0918, Acc: 0.9896\nVal Class Loss: 0.8503, Val Center Loss: 19.3366, Val Acc: 0.7968\nEpoch 26/40\nTrain Loss: 0.3272, Class Loss: 0.1144, Center Loss: 21.2742, Acc: 0.9915\nVal Class Loss: 0.8350, Val Center Loss: 18.5498, Val Acc: 0.7847\nEpoch 27/40\nTrain Loss: 0.3030, Class Loss: 0.1017, Center Loss: 20.1367, Acc: 0.9931\nVal Class Loss: 0.8112, Val Center Loss: 17.8264, Val Acc: 0.8073\nEpoch 28/40\nTrain Loss: 0.2965, Class Loss: 0.1028, Center Loss: 19.3674, Acc: 0.9918\nVal Class Loss: 0.8411, Val Center Loss: 17.4757, Val Acc: 0.7827\nEpoch 29/40\nTrain Loss: 0.2818, Class Loss: 0.0953, Center Loss: 18.6515, Acc: 0.9924\nVal Class Loss: 0.8600, Val Center Loss: 16.4837, Val Acc: 0.7847\nEpoch 30/40\nTrain Loss: 0.2729, Class Loss: 0.0922, Center Loss: 18.0624, Acc: 0.9940\nVal Class Loss: 0.8837, Val Center Loss: 16.3772, Val Acc: 0.7830\nEpoch 31/40\nTrain Loss: 0.2679, Class Loss: 0.0932, Center Loss: 17.4709, Acc: 0.9919\nVal Class Loss: 0.7901, Val Center Loss: 15.9273, Val Acc: 0.8028\nEpoch 32/40\nTrain Loss: 0.2515, Class Loss: 0.0825, Center Loss: 16.9015, Acc: 0.9937\nVal Class Loss: 0.8391, Val Center Loss: 14.9057, Val Acc: 0.7917\nEpoch 33/40\nTrain Loss: 0.2403, Class Loss: 0.0763, Center Loss: 16.4001, Acc: 0.9953\nVal Class Loss: 0.8792, Val Center Loss: 15.2648, Val Acc: 0.7871\nEpoch 34/40\nTrain Loss: 0.2324, Class Loss: 0.0735, Center Loss: 15.8861, Acc: 0.9948\nVal Class Loss: 0.9082, Val Center Loss: 14.6805, Val Acc: 0.7846\nEpoch 35/40\nTrain Loss: 0.2209, Class Loss: 0.0680, Center Loss: 15.2883, Acc: 0.9958\nVal Class Loss: 0.9035, Val Center Loss: 14.2718, Val Acc: 0.7819\nEpoch 36/40\nTrain Loss: 0.2174, Class Loss: 0.0681, Center Loss: 14.9297, Acc: 0.9958\nVal Class Loss: 0.8478, Val Center Loss: 13.7147, Val Acc: 0.7767\nEpoch 37/40\nTrain Loss: 0.2094, Class Loss: 0.0640, Center Loss: 14.5410, Acc: 0.9966\nVal Class Loss: 0.8068, Val Center Loss: 13.3885, Val Acc: 0.7985\nEpoch 38/40\nTrain Loss: 0.2061, Class Loss: 0.0642, Center Loss: 14.1896, Acc: 0.9958\nVal Class Loss: 0.8927, Val Center Loss: 13.1499, Val Acc: 0.7731\nEpoch 39/40\nTrain Loss: 0.1982, Class Loss: 0.0597, Center Loss: 13.8490, Acc: 0.9972\nVal Class Loss: 0.8661, Val Center Loss: 13.4742, Val Acc: 0.7865\nEpoch 40/40\nTrain Loss: 0.1934, Class Loss: 0.0580, Center Loss: 13.5403, Acc: 0.9966\nVal Class Loss: 0.8374, Val Center Loss: 13.0389, Val Acc: 0.7871\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Save the Model","metadata":{}},{"cell_type":"code","source":"# Save the history to a .pkl file\nimport pickle\nwith open('/kaggle/working/cub_history_standard_resnet.pkl', 'wb') as file:\n    pickle.dump(history, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:23:54.766839Z","iopub.execute_input":"2025-05-11T02:23:54.767085Z","iopub.status.idle":"2025-05-11T02:23:54.772880Z","shell.execute_reply.started":"2025-05-11T02:23:54.767069Z","shell.execute_reply":"2025-05-11T02:23:54.772371Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Save the embedding model\nembedding_model.save('/kaggle/working/cub_embedding_model_standard_resnet.keras')\n\n# Save the full model\nfull_model.save('/kaggle/working/cub_full_model_standard_resnet.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:23:56.444174Z","iopub.execute_input":"2025-05-11T02:23:56.444787Z","iopub.status.idle":"2025-05-11T02:23:57.900835Z","shell.execute_reply.started":"2025-05-11T02:23:56.444756Z","shell.execute_reply":"2025-05-11T02:23:57.900266Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Metric and evaluations","metadata":{}},{"cell_type":"code","source":"# Load the embedding model\nimport tensorflow as tf\nembedding_model_loaded = tf.keras.models.load_model('/kaggle/working/cub_embedding_model_standard_resnet.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:26:21.995758Z","iopub.execute_input":"2025-05-11T02:26:21.996068Z","iopub.status.idle":"2025-05-11T02:26:23.152605Z","shell.execute_reply.started":"2025-05-11T02:26:21.996045Z","shell.execute_reply":"2025-05-11T02:26:23.152012Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import LabelEncoder\nfrom matplotlib import cm\nfrom matplotlib.colors import ListedColormap\n\n\n#embedding_model = models.Model(inputs=model.inputs[0], outputs=model.get_layer('embeddings').output)\n\n# Generate embeddings with 'training=False'\nembeddings = embedding_model.predict(X_test)\n\n# Sample image embeddings and labels\n# image_embeddings = ... (your embeddings)\n# labels_np = ... (your string labels)\n\n# Convert string labels to numeric labels\ny_temp_labels = np.argmax(y_temp, axis=1)\nnumeric_labels = LabelEncoder().fit_transform(y_temp_labels)\n\n# Use t-SNE to reduce the embedding space to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d = tsne.fit_transform(embeddings)  # Use your actual embeddings here\n\n# Create a custom color map to support 31 classes\ncolors = cm.get_cmap('tab20b', 31)  # 'tab20b' offers a distinct palette; we can define 31 colors explicitly\n\n# Plot the 2D embeddings with color based on numeric labels\nplt.figure(figsize=(10, 10))\nscatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n                      c=numeric_labels, cmap=colors, s=10)  # Decreased marker size (s=20)\n\n# Add a color legend\nlegend1 = plt.legend(*scatter.legend_elements(), title=\"Person\")\nplt.gca().add_artist(legend1)\n\n# Set plot details\nplt.title('t-SNE of Image Embeddings (Color-coded)')\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:37:50.015285Z","iopub.execute_input":"2025-05-11T02:37:50.015557Z","iopub.status.idle":"2025-05-11T02:38:25.685613Z","shell.execute_reply.started":"2025-05-11T02:37:50.015533Z","shell.execute_reply":"2025-05-11T02:38:25.684454Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/995237945.py:27: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  colors = cm.get_cmap('tab20b', 31)  # 'tab20b' offers a distinct palette; we can define 31 colors explicitly\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4432\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Is 'c' acceptable as PathCollection facecolors?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4433\u001b[0;31m                 \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrgba\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Suppress exception chaining of cache lookup failure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid RGBA argument: {orig_c!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: 0.0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/995237945.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Plot the 2D embeddings with color based on numeric labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n\u001b[0m\u001b[1;32m     32\u001b[0m                       c=numeric_labels, cmap=colors, s=10)  # Decreased marker size (s=20)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         edgecolors=None, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 2862\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   2863\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4594\u001b[0m             \u001b[0morig_edgecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'edgecolor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4595\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4596\u001b[0;31m             self._parse_scatter_color_args(\n\u001b[0m\u001b[1;32m   4597\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4598\u001b[0m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4437\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4438\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4439\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0minvalid_shape_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4440\u001b[0m                     \u001b[0;31m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4441\u001b[0m                     \u001b[0;31m# severe failure => one may appreciate a verbose feedback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 'c' argument has 5794 elements, which is inconsistent with 'x' and 'y' with size 4635."],"ename":"ValueError","evalue":"'c' argument has 5794 elements, which is inconsistent with 'x' and 'y' with size 4635.","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x1000 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import normalized_mutual_info_score\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef evaluation(X, Y, Kset):\n    num = X.shape[0]\n    classN = np.max(Y) + 1\n    kmax = np.max(Kset)\n    recallK = np.zeros(len(Kset))\n    \n    # Compute NMI using KMeans clustering\n    kmeans = KMeans(n_clusters=classN).fit(X)\n    nmi = normalized_mutual_info_score(Y, kmeans.labels_, average_method='arithmetic')\n    \n    # Compute Recall@K\n    sim = X.dot(X.T)\n    minval = np.min(sim) - 1.\n    sim -= np.diag(np.diag(sim))\n    sim += np.diag(np.ones(num) * minval)\n    indices = np.argsort(-sim, axis=1)[:, :kmax]\n    YNN = Y[indices]\n    \n    for i in range(len(Kset)):\n        pos = 0.\n        for j in range(num):\n            if Y[j] in YNN[j, :Kset[i]]:\n                pos += 1.\n        recallK[i] = pos / num\n    \n    return nmi, recallK\n\ndef calculate_metrics(embedding_model, val_data, labels, k_values):\n    # Generate embeddings for the validation set\n    embeddings = embedding_model.predict(val_data)\n    \n    # Convert labels to the appropriate format\n    labels = np.argmax(labels, axis=1)  # Assuming labels are one-hot encoded\n    \n    # Calculate NMI and Recall@K\n    nmi_score, recall_scores = evaluation(embeddings, labels, k_values)\n    \n    return recall_scores, nmi_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:40:45.206908Z","iopub.execute_input":"2025-05-11T02:40:45.207419Z","iopub.status.idle":"2025-05-11T02:40:45.357103Z","shell.execute_reply.started":"2025-05-11T02:40:45.207397Z","shell.execute_reply":"2025-05-11T02:40:45.356407Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"k_values = [1, 2, 4, 8, 16, 32, 64, 128]\nrecall_scores_test, nmi_score_test = calculate_metrics(embedding_model, X_test, y_test, k_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:40:47.251229Z","iopub.execute_input":"2025-05-11T02:40:47.251900Z","iopub.status.idle":"2025-05-11T02:41:33.283934Z","shell.execute_reply.started":"2025-05-11T02:40:47.251878Z","shell.execute_reply":"2025-05-11T02:41:33.283288Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(recall_scores_test, nmi_score_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:41:40.128384Z","iopub.execute_input":"2025-05-11T02:41:40.128947Z","iopub.status.idle":"2025-05-11T02:41:40.132834Z","shell.execute_reply.started":"2025-05-11T02:41:40.128927Z","shell.execute_reply":"2025-05-11T02:41:40.132148Z"}},"outputs":[{"name":"stdout","text":"[0.75080906 0.81445523 0.84983819 0.88263215 0.91995685 0.9562028\n 0.97540453 0.98532902] 0.8649237484230071\n","output_type":"stream"}],"execution_count":22}]}